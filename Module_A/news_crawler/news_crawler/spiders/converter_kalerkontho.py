import csv
import json
import os
import re
import html
from datetime import datetime
from urllib.parse import urlparse, urlunparse
from bs4 import BeautifulSoup

# --------------------------------------------------
# Paths
# --------------------------------------------------
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
input_csv = os.path.join(BASE_DIR, "kalerkantho_raw_documents.csv")
output_jsonl = os.path.join(BASE_DIR, "bangla_corpus.jsonl")

DEFAULT_AUTHOR = "‡¶®‡¶ø‡¶ú‡¶∏‡ßç‡¶¨ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡ßá‡¶¶‡¶ï"

# --------------------------------------------------
# Bangla date helpers
# --------------------------------------------------
BN_DIGITS = str.maketrans("0123456789", "‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ")

BN_MONTHS = {
    "January": "‡¶ú‡¶æ‡¶®‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø",
    "February": "‡¶´‡ßá‡¶¨‡ßç‡¶∞‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø",
    "March": "‡¶Æ‡¶æ‡¶∞‡ßç‡¶ö",
    "April": "‡¶è‡¶™‡ßç‡¶∞‡¶ø‡¶≤",
    "May": "‡¶Æ‡ßá",
    "June": "‡¶ú‡ßÅ‡¶®",
    "July": "‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á",
    "August": "‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü",
    "September": "‡¶∏‡ßá‡¶™‡ßç‡¶ü‡ßá‡¶Æ‡ßç‡¶¨‡¶∞",
    "October": "‡¶Ö‡¶ï‡ßç‡¶ü‡ßã‡¶¨‡¶∞",
    "November": "‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞",
    "December": "‡¶°‡¶ø‡¶∏‡ßá‡¶Æ‡ßç‡¶¨‡¶∞"
}

def to_bangla_date(date_str: str) -> str:
    """
    Input: 2026-01-19 08:37:00
    Output: ‡ßß‡ßØ ‡¶ú‡¶æ‡¶®‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø ‡ß®‡ß¶‡ß®‡ß¨, ‡ß¶‡ßÆ:‡ß©‡ß≠
    """
    try:
        dt = datetime.strptime(date_str.strip(), "%Y-%m-%d %H:%M:%S")
    except:
        return date_str.strip()

    eng = dt.strftime("%d %B %Y, %H:%M")

    for en, bn in BN_MONTHS.items():
        eng = eng.replace(en, bn)

    return eng.translate(BN_DIGITS)

# --------------------------------------------------
# Helpers
# --------------------------------------------------
def normalize_url(url: str) -> str:
    if not url:
        return ""

    parsed = urlparse(url.strip())
    normalized = parsed._replace(
        scheme="https",
        query="",
        fragment=""
    )
    return urlunparse(normalized).rstrip("/")


def clean_html(text: str) -> str:
    if not text:
        return ""

    text = html.unescape(text)
    soup = BeautifulSoup(text, "lxml")

    for tag in soup.find_all([
        "script", "style", "img", "figure", "iframe",
        "svg", "button", "input", "form", "nav"
    ]):
        tag.decompose()

    cleaned = soup.get_text(separator=" ")
    cleaned = re.sub(r"\s+", " ", cleaned).strip()

    return cleaned

# --------------------------------------------------
# 1Ô∏è‚É£ Load existing URLs (dedup by URL ONLY)
# --------------------------------------------------
existing_urls = set()

if os.path.exists(output_jsonl):
    with open(output_jsonl, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                url = normalize_url(obj.get("url", ""))
                if url:
                    existing_urls.add(url)
            except:
                pass

print(f"üîç Existing documents: {len(existing_urls)}")

# --------------------------------------------------
# 2Ô∏è‚É£ Convert CSV ‚Üí JSONL
# --------------------------------------------------
added = 0
skipped_dup = 0
skipped_empty = 0
skipped_bad = 0

with open(input_csv, "r", encoding="utf-8", newline="") as fin, \
     open(output_jsonl, "a", encoding="utf-8") as fout:

    reader = csv.DictReader(fin)

    for row in reader:
        url = normalize_url(row.get("url", ""))

        if not url:
            skipped_bad += 1
            continue

        if url in existing_urls:
            skipped_dup += 1
            continue

        body_clean = clean_html(row.get("body", ""))

        if not body_clean:
            skipped_empty += 1
            continue

        author = row.get("author", "").strip()
        if not author:
            author = DEFAULT_AUTHOR

        doc = {
            "title": row.get("title", "").strip(),
            "body": body_clean,
            "url": url,
            "date": to_bangla_date(row.get("date", "")),
            "language": "bn",
            "author": author,
            "tokens": len(body_clean.split()),
            "section": row.get("category", "").lower()
        }

        fout.write(json.dumps(doc, ensure_ascii=False) + "\n")
        existing_urls.add(url)
        added += 1

# --------------------------------------------------
# Report
# --------------------------------------------------
print("‚úÖ Conversion finished")
print(f"‚ûï Added: {added}")
print(f"‚è≠Ô∏è Skipped (duplicates): {skipped_dup}")
print(f"‚è≠Ô∏è Skipped (empty body): {skipped_empty}")
print(f"‚è≠Ô∏è Skipped (bad rows): {skipped_bad}")

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIR System (Module B + Module C)\n",
        "\n",
        "## Overview\n",
        "This notebook integrates **Module B (Query Processing)** and **Module C (Search Engine)** into a unified pipeline.\n",
        "It supports:\n",
        "- **Query Processing**: Language Detection, NLLB Translation, Spell Correction, NER.\n",
        "- **Search Metrics**: Fuzzy, Semantic (LaBSE), BM25.\n",
        "- **Cross-Lingual Analysis**: Retrieving Top 5 Native + Top 2 Translated documents.\n",
        "\n",
        "## Setup\n",
        "Upload files to `/content/`:\n",
        "- `bangla_corpus.jsonl`, `english_corpus.jsonl`\n",
        "- `bangla_embeddings.npy`, `english_embeddings.npy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "!pip install -q transformers sentence-transformers dateparser numpy torch pyspellchecker rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import difflib\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import dateparser\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from spellchecker import SpellChecker\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "print('Libraries imported.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ==========================================\n",
        "# MODULE B: Query Processor (Heavy Models)\n",
        "# ==========================================\n",
        "This module handles the Query Understanding pipeline:\n",
        "1. **Detection**: Unicode check.\n",
        "2. **Correction**: Fuzzy spell checker.\n",
        "3. **Translation**: NLLB-200 (600M).\n",
        "4. **Parsing**: NER and DateParser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QueryProcessor:\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Advanced Query Processor...\")\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Device: {self.device}\")\n",
        "\n",
        "        # 1. Fuzzy Spell Checker\n",
        "        self.spell = SpellChecker()\n",
        "\n",
        "        # 2. Load Translation Model (NLLB-200)\n",
        "        print(\"Loading NLLB-200 Translation Model (600M)... this may take a moment.\")\n",
        "        self.trans_model_name = 'facebook/nllb-200-distilled-600M'\n",
        "        self.trans_tokenizer = AutoTokenizer.from_pretrained(self.trans_model_name)\n",
        "        self.trans_model = AutoModelForSeq2SeqLM.from_pretrained(self.trans_model_name).to(self.device)\n",
        "\n",
        "        # 3. Load NER Model (Multilingual XLM-R for Entities)\n",
        "        print(\"Loading NER Pipeline (Multilingual XLM-R)...\")\n",
        "        self.ner_model_name = 'Davlan/xlm-roberta-base-ner-hrl'\n",
        "        self.ner_pipeline = pipeline('ner', model=self.ner_model_name, aggregation_strategy='simple', device=0 if self.device == 'cuda' else -1)\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        # Simple robust Unicode check\n",
        "        bangla_chars = [c for c in text if '\\u0980' <= c <= '\\u09ff']\n",
        "        if len(bangla_chars) > len(text) * 0.2:\n",
        "            return 'bn'\n",
        "        return 'en'\n",
        "\n",
        "    def correct_spelling(self, text, lang):\n",
        "        if lang == 'en':\n",
        "            words = text.split()\n",
        "            corrected = []\n",
        "            for word in words:\n",
        "                if word.lower() not in self.spell:\n",
        "                   cand = self.spell.correction(word)\n",
        "                   corrected.append(cand if cand else word)\n",
        "                else:\n",
        "                   corrected.append(word)\n",
        "            return \" \".join(corrected)\n",
        "        return text\n",
        "\n",
        "    def translate(self, text, src_lang, tgt_lang):\n",
        "        lang_map = {'bn': 'ben_Beng', 'en': 'eng_Latn'}\n",
        "        src_code = lang_map.get(src_lang)\n",
        "        tgt_code = lang_map.get(tgt_lang)\n",
        "        if not src_code or not tgt_code: return text\n",
        "\n",
        "        inputs = self.trans_tokenizer(text, return_tensors='pt').to(self.device)\n",
        "        forced_bos_token_id = self.trans_tokenizer.convert_tokens_to_ids(tgt_code)\n",
        "        \n",
        "        translated_tokens = self.trans_model.generate(\n",
        "            **inputs, forced_bos_token_id=forced_bos_token_id, max_length=128\n",
        "        )\n",
        "        result = self.trans_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "        return result\n",
        "    \n",
        "    def extract_entities(self, text):\n",
        "        dates = dateparser.parse(text)\n",
        "        date_str = str(dates.date()) if dates else None\n",
        "        entities = []\n",
        "        try:\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "            for ent in ner_results:\n",
        "                if ent['score'] > 0.5:\n",
        "                    entities.append((ent['word'], ent['entity_group']))\n",
        "        except: pass\n",
        "        return {'dates': date_str, 'ner': entities}\n",
        "\n",
        "    def process(self, query):\n",
        "        lang = self.detect_language(query)\n",
        "        corrected_query = self.correct_spelling(query, lang)\n",
        "        norm_query = corrected_query.strip().lower()\n",
        "        target_lang = 'en' if lang == 'bn' else 'bn'\n",
        "        translated_query = self.translate(norm_query, lang, target_lang)\n",
        "        entities = self.extract_entities(norm_query)\n",
        "        return {\n",
        "            'original': query,\n",
        "            'corrected': corrected_query,\n",
        "            'lang': lang,\n",
        "            'translated': translated_query,\n",
        "            'entities': entities\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ==========================================\n",
        "# MODULE C: Search Engine (Hybrid Matcher)\n",
        "# ==========================================\n",
        "This module handles the retrieval logic using 3 metrics:\n",
        "1. **Fuzzy**: Levenshtein Distance + N-gram Containment.\n",
        "2. **Semantic**: LaBSE Cosine Similarity.\n",
        "3. **BM25**: Probabilistic Term Matching (New Integration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridMatcher:\n",
        "    def __init__(self, bangla_corpus_path, english_corpus_path, \n",
        "                 bangla_emb_path=None, english_emb_path=None):\n",
        "        # Load Corpora\n",
        "        self.bangla_corpus = self._load_corpus(bangla_corpus_path)\n",
        "        self.english_corpus = self._load_corpus(english_corpus_path)\n",
        "        \n",
        "        # Load Embeddings (Semantic)\n",
        "        self.bangla_embeddings = None\n",
        "        self.english_embeddings = None\n",
        "        self.model = None\n",
        "\n",
        "        try:\n",
        "            print('Loading LaBSE model...')\n",
        "            self.model = SentenceTransformer('sentence-transformers/LaBSE')\n",
        "            if bangla_emb_path and os.path.exists(bangla_emb_path):\n",
        "                self.bangla_embeddings = np.load(bangla_emb_path)\n",
        "            if english_emb_path and os.path.exists(english_emb_path):\n",
        "                self.english_embeddings = np.load(english_emb_path)\n",
        "            print('Embeddings loaded.')\n",
        "        except Exception as e:\n",
        "            print(f'Error loading LaBSE: {e}')\n",
        "\n",
        "        # Initialize BM25 (New)\n",
        "        print('Initializing BM25 Indices...')\n",
        "        self.bm25_bangla = self._build_bm25(self.bangla_corpus)\n",
        "        self.bm25_english = self._build_bm25(self.english_corpus)\n",
        "        print('BM25 Indices ready.')\n",
        "\n",
        "    def _load_corpus(self, path):\n",
        "        if not os.path.exists(path): return []\n",
        "        docs = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try: docs.append(json.loads(line))\n",
        "                except: continue\n",
        "        return docs\n",
        "    \n",
        "    def _build_bm25(self, corpus):\n",
        "        tokenized_corpus = [self._tokenize_list(doc.get('title', '') + \" \" + doc.get('body', '')) for doc in corpus]\n",
        "        return BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    def _tokenize_list(self, text):\n",
        "        return text.lower().split()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return set(text.lower().split())\n",
        "\n",
        "    def _get_ngrams(self, text, n=3):\n",
        "        text = text.lower()\n",
        "        return [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "\n",
        "    # -- Metric 1: Fuzzy --\n",
        "    def get_fuzzy_score(self, query, title, body):\n",
        "        lev = difflib.SequenceMatcher(None, query.lower(), title.lower()).ratio()\n",
        "        \n",
        "        ngrams_q = self._get_ngrams(query)\n",
        "        ngrams_t = self._get_ngrams(title)\n",
        "        containment = 0.0\n",
        "        if ngrams_q and ngrams_t:\n",
        "            c_q = Counter(ngrams_q)\n",
        "            c_t = Counter(ngrams_t)\n",
        "            containment = sum((c_q & c_t).values()) / len(ngrams_q)\n",
        "            \n",
        "        title_score = max(lev, containment)\n",
        "        if query.lower() in title.lower(): title_score = 1.0\n",
        "        \n",
        "        # Jaccard body\n",
        "        tokens_q = self._tokenize(query)\n",
        "        tokens_b = self._tokenize(body)\n",
        "        jaccard = 0.0\n",
        "        if tokens_q and tokens_b:\n",
        "            jaccard = len(tokens_q & tokens_b) / len(tokens_q | tokens_b)\n",
        "            \n",
        "        return (title_score * 0.8) + (jaccard * 0.2)\n",
        "\n",
        "    # -- Search Logic --\n",
        "    def search(self, query, language='bn', top_k=10, mode='hybrid'):\n",
        "        corpus = self.bangla_corpus if language == 'bn' else self.english_corpus\n",
        "        embeddings = self.bangla_embeddings if language == 'bn' else self.english_embeddings\n",
        "        bm25 = self.bm25_bangla if language == 'bn' else self.bm25_english\n",
        "        \n",
        "        # 1. Semantic Scores\n",
        "        semantic_scores = None\n",
        "        if self.model and embeddings is not None:\n",
        "            try:\n",
        "                query_emb = self.model.encode(query, convert_to_tensor=True)\n",
        "                semantic_scores = util.cos_sim(query_emb, embeddings)[0].cpu().numpy()\n",
        "            except: pass\n",
        "            \n",
        "        # 2. BM25 Scores\n",
        "        bm25_scores = bm25.get_scores(self._tokenize_list(query))\n",
        "        # Normalize BM25 (Softmax or MinMax). Simple MinMax here for safety.\n",
        "        if len(bm25_scores) > 0 and max(bm25_scores) > 0:\n",
        "            bm25_scores = bm25_scores / max(bm25_scores)\n",
        "\n",
        "        input_results = []\n",
        "        \n",
        "        for idx, doc in enumerate(corpus):\n",
        "            title = doc.get('title', '')\n",
        "            body = doc.get('body', '')\n",
        "            \n",
        "            # Extract individual scores\n",
        "            fuzzy_score = self.get_fuzzy_score(query, title, body)\n",
        "            \n",
        "            sem_score = 0.0\n",
        "            if semantic_scores is not None and idx < len(semantic_scores):\n",
        "                sem_score = max(0.0, float(semantic_scores[idx]))\n",
        "            \n",
        "            bm25_val = 0.0\n",
        "            if len(bm25_scores) > idx:\n",
        "                bm25_val = float(bm25_scores[idx])\n",
        "                \n",
        "            # Calculate Final Score based on Mode\n",
        "            final_score = 0.0\n",
        "            if mode == 'fuzzy':\n",
        "                final_score = fuzzy_score\n",
        "            elif mode == 'semantic':\n",
        "                final_score = sem_score\n",
        "            elif mode == 'bm25':\n",
        "                final_score = bm25_val\n",
        "            else: # Hybrid\n",
        "                # Formula: 0.3 * BM25 + 0.5 * Semantic + 0.2 * Fuzzy\n",
        "                final_score = (bm25_val * 0.3) + (sem_score * 0.5) + (fuzzy_score * 0.2)\n",
        "\n",
        "            if final_score > 0.1:\n",
        "                input_results.append({\n",
        "                    'doc': doc,\n",
        "                    'score': final_score,\n",
        "                    'metrics': {'fuzzy': fuzzy_score, 'semantic': sem_score, 'bm25': bm25_val}\n",
        "                })\n",
        "        \n",
        "        input_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return input_results[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Components\n",
        "processor = QueryProcessor()\n",
        "matcher = HybridMatcher(\n",
        "    bangla_corpus_path='/content/bangla_corpus.jsonl',\n",
        "    english_corpus_path='/content/english_corpus.jsonl',\n",
        "    bangla_emb_path='/content/bangla_embeddings.npy',\n",
        "    english_emb_path='/content/english_embeddings.npy'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# ANALYSIS: Top 5 Native + Top 2 Translated\n",
        "# ==========================================\n",
        "\n",
        "def analyze_query(user_query):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SEARCH ANALYSIS FOR: '{user_query}'\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # 1. Process Query\n",
        "    p = processor.process(user_query)\n",
        "    use_q = p['corrected']\n",
        "    native_lang = p['lang']  # 'en' or 'bn'\n",
        "    trans_q = p['translated']\n",
        "    target_lang = 'bn' if native_lang == 'en' else 'en'\n",
        "    \n",
        "    print(f\"[Processing] Lang: {native_lang} | Corrected: '{use_q}' | Translated ({target_lang}): '{trans_q}'\")\n",
        "    \n",
        "    # 2. Search NATIVE Language (Top 5)\n",
        "    print(f\"\\n>>> TOP 5 RESULTS IN NATIVE LANGUAGE ({native_lang.upper()}):\")\n",
        "    res_native = matcher.search(use_q, language=native_lang, top_k=5, mode='hybrid')\n",
        "    if not res_native:\n",
        "        print(\"No relevant documents found.\")\n",
        "    for i, r in enumerate(res_native, 1):\n",
        "        print(f\"{i}. [{r['score']:.4f}] {r['doc'].get('title', 'N/A')}\")\n",
        "        \n",
        "    # 3. Search TRANSLATED Language (Top 2)\n",
        "    print(f\"\\n>>> TOP 2 RESULTS IN TRANSLATED LANGUAGE ({target_lang.upper()}):\")\n",
        "    # Note: We use the *translated query* here\n",
        "    res_trans = matcher.search(trans_q, language=target_lang, top_k=2, mode='hybrid')\n",
        "    if not res_trans:\n",
        "        print(\"No relevant documents found.\")\n",
        "    for i, r in enumerate(res_trans, 1):\n",
        "        print(f\"{i}. [{r['score']:.4f}] {r['doc'].get('title', 'N/A')}\")\n",
        "            \n",
        "    # Note: We removed the plotting as requested"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# EXPANDED DEMO: 40 Candidates -> Select Best\n",
        "# ==========================================\n",
        "\n",
        "# 1. Define 40 Candidate Queries (20 English, 20 Bangla)\n",
        "candidate_queries = [\n",
        "    # --- 20 English Queries ---\n",
        "    \"coronavirus vaccine\", \"election results 2024\", \"dhaka traffic jam\", \"metro rail schedule\",\n",
        "    \"bangladesh cricket team news\", \"global warming effects\", \"inflation rate in bangladesh\",\n",
        "    \"stock market crash\", \"hospitals in dhaka\", \"best schools in chittagong\",\n",
        "    \"mobile banking security\", \"internet speed test\", \"parliament session live\",\n",
        "    \"fifa world cup winners\", \"rohingya refugee crisis updates\", \"hilsha fish export price\",\n",
        "    \"padma bridge toll rate\", \"startup ecosystem in dhaka\", \"flood situation in sylhet\",\n",
        "    \"dengue fever symptoms\",\n",
        "\n",
        "    # --- 20 Bangla Queries ---\n",
        "    \"করোনাভাইরাস টিকা\", \"বাংলাদেশ নির্বাচন ফলাফল\", \"ঢাকা ট্রাফিক জ্যাম\", \"মেট্রো রেল সময়সূচী\",\n",
        "    \"বাংলাদেশ ক্রিকেট দলের খবর\", \"বিশ্ব উষ্ণায়নের প্রভাব\", \"বাংলাদেশে মুদ্রাস্ফীতির হার\",\n",
        "    \"শেয়ার বাজার ধস\", \"ঢাকার হাসপাতাল সমূহ\", \"চট্টগ্রামের সেরা স্কুল\",\n",
        "    \"মোবাইল ব্যাংকিং নিরাপত্তা\", \"ইন্টারনেট গতি পরীক্ষা\", \"সংসদ অধিবেশন লাইভ\",\n",
        "    \"ফিফা বিশ্বকাপ বিজয়ী\", \"রোহিঙ্গা সংকট আপডেট\", \"ইলিশ মাছ রপ্তানি দাম\",\n",
        "    \"পদ্মা সেতু টোল\", \"স্টার্টআপ ইকোসিস্টেম\", \"সিলেটে বন্যা পরিস্থিতি\",\n",
        "    \"ডেঙ্গু জ্বরের লক্ষণ\"\n",
        "]\n",
        "\n",
        "def filter_best_queries(candidates, top_k_per_lang=5):\n",
        "    print(f\"Filtering top {top_k_per_lang*2} queries from {len(candidates)} candidates...\")\n",
        "    scored_candidates = []\n",
        "    \n",
        "    for q in candidates:\n",
        "        # Lightweight check: get correct lang and run a quick search\n",
        "        # We use processor just for lang detection here\n",
        "        lang = processor.detect_language(q)\n",
        "        \n",
        "        # Standardize for matching\n",
        "        corrected = processor.correct_spelling(q, lang)\n",
        "        \n",
        "        # Get Score (using Hybrid mode for selection)\n",
        "        res = matcher.search(corrected, language=lang, top_k=1, mode='hybrid')\n",
        "        top_score = res[0]['score'] if res else 0.0\n",
        "        \n",
        "        scored_candidates.append({\n",
        "            'query': q,\n            'score': top_score,\n            'lang': lang\n        })\n        \n    # Sort and Select\n    en_candidates = [x for x in scored_candidates if x['lang'] == 'en']\n    bn_candidates = [x for x in scored_candidates if x['lang'] == 'bn']\n    \n    # Get Top-K highest scoring for each language\n    best_en = sorted(en_candidates, key=lambda x: x['score'], reverse=True)[:top_k_per_lang]\n    best_bn = sorted(bn_candidates, key=lambda x: x['score'], reverse=True)[:top_k_per_lang]\n    \n    return best_en + best_bn\n",
        "\n",
        "# 2. Run Filtering\n",
        "top_10_queries = filter_best_queries(candidate_queries)\n",
        "\n",
        "print(f\"\\nSelected Top 10 High-Relevance Queries:\")\n",
        "for item in top_10_queries:\n",
        "    print(f\"- [{item['lang'].upper()}] {item['query']} (Confidence: {item['score']:.4f})\")\n",
        "\n",
        "# 3. Run Deep-Dive Analysis on Finalists\n",
        "for item in top_10_queries:\n",
        "    analyze_query(item['query'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
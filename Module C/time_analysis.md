Here are the completed tables with the calculated totals and the breakdown of your efficiency upgrade.

### **Table 1: Query Processing Latency**
| Component | Min (ms) | Avg (ms) | Max (ms) | % of Total Avg Time |
| :--- | :--- | :--- | :--- | :--- |
| Language Detection | 0.00397 | 0.00600 | 0.00925 | 0.004% |
| Normalization | 0.02073 | 0.05519 | 0.07718 | 0.035% |
| Named Entity Recognition (NER) | 13.71878 | 14.73686 | 19.27755 | 9.310% |
| Query Expansion | 0.00544 | 0.00998 | 0.01796 | 0.006% |
| **Total Query Processing** | **13.74892** | **14.80804** | **19.38193** | **9.355%** |

---

### **Table 2: Retrieval & Ranking Latency**
| Component | Min (ms) | Avg (ms) | Max (ms) | % of Total Avg Time |
| :--- | :--- | :--- | :--- | :--- |
| Semantic Embedding (LaBSE) | 10.45 | 11.01 | 12.25 | 6.956% |
| BM25 Search | 17.16 | 35.35 | 58.57 | 22.333% |
| Semantic Similarity | 1.26 | 1.37 | 1.72 | 0.865% |
| **Fuzzy Search (Top-100)** | **50.19** | **94.33** | **158.03** | **59.594%** |
| Score Fusion | 0.17 | 0.22 | 0.27 | 0.139% |
| Global Ranking | 1.01 | 1.20 | 1.29 | 0.758% |
| **Total Retrieval Time** | **80.24** | **143.48** | **232.13** | **90.645%** |

---

### **Table 3: Final System Totals**
| Metric | Min (ms) | Avg (ms) | Max (ms) |
| :--- | :--- | :--- | :--- |
| **End-to-End Latency** | **93.989 ms** | **158.288 ms** | **251.512 ms** |

---

### **Efficiency Upgrade: The "Top-100 Re-ranking" Strategy**

The most significant efficiency upgrade implemented in the retriever was the transition from **Global Fuzzy Search** to **Candidate-Limited Fuzzy Re-ranking**.

#### **The Problem (Before Upgrade):**
In a standard implementation, Fuzzy Search (Levenshtein distance and N-Gram containment) is computationally expensive ($O(n \times m)$ complexity per document). Running this across the entire Bangla and English corpora (thousands of documents) caused the system latency to spike into the **2,000ms â€“ 3,500ms** range, making the search engine feel sluggish and non-responsive.

#### **The Solution (The Upgrade):**
We modified the `search()` workflow to treat Fuzzy Search as a **Re-ranker** rather than a primary retriever. 
1.  **Stage 1 (Filtering):** We use BM25 (Sparse) and LaBSE (Dense) to scan the entire corpus. These methods are optimized for speed and return a "Candidate Set" of the most relevant results very quickly.
2.  **Stage 2 (Re-ranking):** We limit the computationally heavy Fuzzy Search to **only the Top 100 candidates** identified in Stage 1. 

#### **The Result:**
*   **Latency Reduction:** Fuzzy Search average time dropped from **~2,480ms** to **~94ms** (a **96% speed improvement**).
*   **Total System Speed:** The entire system now runs in under **160ms** on average, achieving real-time performance levels.
*   **Quality Preservation:** Because Fuzzy Search is most useful for correcting minor typos or script mismatches in high-relevance results, applying it only to the Top 100 candidates preserves the accuracy boost while eliminating the massive computational overhead.